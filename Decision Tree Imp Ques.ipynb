{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d918737d",
   "metadata": {},
   "source": [
    "### 1.Explain the Decision Tree algorithm in detail.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a2c555f4",
   "metadata": {},
   "source": [
    "A decision tree is a supervised machine learning algorithm, which is utilized for both classification and regression tasks. It has a hierarchical, tree structure, which consists of a root node, branches, internal nodes and leaf nodes.They can be used for both linear and non-linear data, but they are mostly used for non-linear data.\n",
    "\n",
    "The Root Node: Is the node that starts the graph. In a normal decision tree it evaluates the variable that best splits the data.\n",
    "Intermediate nodes: These are nodes where variables are evaluated but which are not the final nodes where predictions are made.\n",
    "Leaf nodes: These are the final nodes of the tree, where the predictions of a category or a numerical value are made"
   ]
  },
  {
   "cell_type": "raw",
   "id": "169d8713",
   "metadata": {},
   "source": [
    " decision trees are built by recursively splitting our training samples using the features from the data that work best for the specific task. This is done by evaluating certain metrics, like the Gini index or the Entropy for categorical decision trees, or the Residual or Mean Squared Error for regression trees."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d01c9e84",
   "metadata": {},
   "source": [
    "Entropy: It gives the measure of impurity or randomness in the data.\n",
    "It is given by:\n",
    "\n",
    "Entropy= — P(class 1) x Log(P(class 1)) — P(class 2) x Log(P(class 2))\n",
    "\n",
    "where P denotes the probability.\n",
    "\n",
    "If there are two classes, class 1 and class 2, of equal numbers, i.e, the number of entries of class 1 is equal to the number of entries of class 2, and we randomly select an entry, it will belong to any class 1 or 2 with a 50% probability each. In such cases, the entropy will be high\n",
    "\n",
    "If a certain dataset has all data belonging to either class 1 or class 2, the entropy obtained is 0, as in that case P(class1) or P(class2) will be equal to 0. If P(class1)=0 then P(class2) should be equal to 1.So, it is evident that the entropy will be high if we have impure or mixed class labels in a dataset."
   ]
  },
  {
   "cell_type": "raw",
   "id": "cb40035f",
   "metadata": {},
   "source": [
    "If in a dataset there are a total of 20 entries or rows, and 14 of them belongs to label 1 and 6 of them belongs to label 2, the entropy will be equal to:\n",
    "\n",
    "= — P(label 1).log(P(label 1)) — P(label 2).log(P(label 2))\n",
    "\n",
    "= — 14/20.log(14/20) — 6/20.log(6/20)\n",
    "\n",
    "=0.880"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ffca5c70",
   "metadata": {},
   "source": [
    "Information Gain: The information gain is the decrease in the entropy after the dataset is split on the basis of an attribute. Constructing a decision tree depends on finding the attribute that returns the highest information gain. It helps in choosing which feature or attribute will be used to create the deciding internal node at a particular point.\n",
    "\n",
    "It is given by:\n",
    "\n",
    "Information gain=Entropy(s) — [(Weighted average) x (Entropy of each feature)\n",
    "I.G for each feature is calculated and\n",
    "the feature which has highest information gain value will be used  to form the root node."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ad34eaec",
   "metadata": {},
   "source": [
    "Gini index: Gini Index is also a measure of impurity used to build a decision tree using the CART mechanism.\n",
    "\n",
    "It is given by:\n",
    "\n",
    "Gini Impurity= 1 — (Probability of ‘Class 1’)² — (Probability of ‘Class 2’)²\n",
    "\n",
    "if every entry in a dataset belongs to only 1 class, i.e, either class 1 or class 2,then gini index will be 0 \n",
    "we can see for a pure distribution the Gini impurity index is 0.\n",
    "range of G.I. is 0 to 0.5\n",
    "\n",
    "Gini Impurity index can also be used to decide which feature should be used to create the condition node. The feature that results in a smaller Gini impurity index is chosen to create the internal condition node at that point."
   ]
  },
  {
   "cell_type": "raw",
   "id": "714ae303",
   "metadata": {},
   "source": [
    "###  the condition in case of continuous numerical features"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ffa88439",
   "metadata": {},
   "source": [
    "The process to find the best suitable value to create a condition is pretty easy. First, we sort the dataset based on the numerical valued feature in an ascending manner. Next, we find the average of the adjacent pairs of numeric values."
   ]
  },
  {
   "cell_type": "raw",
   "id": "be73d53f",
   "metadata": {},
   "source": [
    "Next, we fit the average values in the condition to check which provides the minimum Gini Impurity index or which provides the maximum information gain, based on the methods we are using. The average value serves as the cutoff value in our condition formed for the feature.\n",
    "\n",
    "Similarly, for discrete value-based or class-based features, we try and fit every value present in the set and create the condition. Finally, we select the value or condition that gives the minimum Gini impurity index"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ad4bc229",
   "metadata": {},
   "source": [
    "Now, we have seen we consider the Gini impurity index or Information gain to decide which condition we should consider. In some cases, some features show no improvement at all or 0 information gain, such features are never used in a decision tree. This is how the decision tree does automatic feature selection."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e5d213e8",
   "metadata": {},
   "source": [
    "One of the chief challenges of the decision tree is that it results in overfitting the data. This is mostly because of the fact, that it creates a condition-based approach to the training data. So, it fits very tightly on the training data. Now, the more the conditions applied on the train data in the tree, the deeper it grows, the tighter it fits the data. But after a point, it starts taking into consideration very small changes on some features, that give very low information gain, mostly these points destroy the generalization of the model and behave like outliers on the test data. We can restrict these conditions by using a threshold on the information gain, such that, if the information gain provided by the condition is less than a given value, we won’t consider the condition. This partially prevents overfitting and helps create a generalized model."
   ]
  },
  {
   "cell_type": "raw",
   "id": "76a18839",
   "metadata": {},
   "source": [
    " In  non-linear data distribution cases, Regression Trees are used.\n",
    " In regression trees, the leaves represent a continuous numeric value in contrast to classification trees which normally represent boolean or discrete values at the leaves.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e8ea2816",
   "metadata": {},
   "source": [
    "Pruning\n",
    "The method is used in trees to reduce overfitting. It is a procedure in which nodes and branches of trees are reduced in order to reduce the depth of the tree."
   ]
  },
  {
   "cell_type": "raw",
   "id": "98d78d0f",
   "metadata": {},
   "source": [
    "There are basically two types of Pruning:\n",
    "\n",
    "Pre-Pruning\n",
    "Post Pruning\n",
    "In Pre-pruning, we set parameters(hyper) like ‘min_samples’, ‘max_depth’, and ‘max_leaves’ during the creation of the tree. All of the parameters need hyperparameter tuning and found using cross-validation and grid-search methods. It restricts the tree to a certain depth or a certain number of leaves.\n",
    "\n",
    "Post-pruning methods are mostly done after the tree is already formed. Cost complexity pruning is the most used post-pruning approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c846bd70",
   "metadata": {},
   "source": [
    "## 2. What are the Steps for Making a decision tree?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0c388770",
   "metadata": {},
   "source": [
    "1.Choose the initial dataset with the feature and target attributes defined.\n",
    "2.Calculate the Information gain and Entropy for each attribute.\n",
    "3.Pick the attribute with the highest information gain, and make it the decision root node.\n",
    "4.Calculate the information gain for the remaining attributes.\n",
    "5.Create recurring child nodes by starting splitting at the decision node (i.e for various values of the decision node, create separate child nodes).\n",
    "6.Repeat this process until all the attributes are covered.\n",
    "7.Prune the Tree to prevent overfitting."
   ]
  },
  {
   "cell_type": "raw",
   "id": "90fbf3ed",
   "metadata": {},
   "source": [
    "A decision tree before starting usually considers the entire data as a root. Then on particular condition, it starts splitting by means of branches or internal nodes and makes a decision until it produces the outcome as a leaf. Only one important thing to know is it reduces impurity present in the attributes and simultaneously gains information to achieve the proper outcomes while building a tree."
   ]
  },
  {
   "cell_type": "raw",
   "id": "10ada3cf",
   "metadata": {},
   "source": [
    "Two Types of Decision Tree:\n",
    "1.Classification\n",
    "Classification trees are applied on data when the outcome is discrete in nature or is categorical such as presence or absence of students in a class, a person died or survived, approval of loan etc. 2.Regression\n",
    "regression trees are used when the outcome of the data is continuous in nature such as prices, age of a person, length of stay in a hotel, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2152dbd7",
   "metadata": {},
   "source": [
    "## 3. What are the Algorithms used in the Decision Tree?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "68f0d71c",
   "metadata": {},
   "source": [
    "There are multiple algorithms which are used for building Decision Trees. Some of them are :\n",
    "\n",
    "ID3 (Iterative Dichotomiser 3)\n",
    "C4.5 (successor of ID3)\n",
    "CART (Classification And Regression Tree)\n",
    "Chi-square automatic interaction detection (CHAID). Performs multi-level splits when computing classification trees.\n",
    "MARS: extends decision trees to handle numerical data better."
   ]
  },
  {
   "cell_type": "raw",
   "id": "bb07e479",
   "metadata": {},
   "source": [
    "ID3 generates a tree by considering the whole set S as the root node. It then iterates on every attribute and splits the data into fragments known as subsets to calculate the entropy or the information gain of that attribute. After splitting, the algorithm recourses on every subset by taking those attributes which were not taken before into the iterated ones. It is not an ideal algorithm as it generally overfits the data and on continuous variables, splitting the data can be time consuming"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ec87efc0",
   "metadata": {},
   "source": [
    "C4.5 It is quite advanced compared to ID3 as it considers the data which are classified samples. The splitting is done based on the normalized information gain and the feature having the highest information gain makes the decision. Unlike ID3, it can handle both continuous and discrete attributes very efficiently and after building a tree, it undergoes pruning by removing all the branches having low importance."
   ]
  },
  {
   "cell_type": "raw",
   "id": "533862f4",
   "metadata": {},
   "source": [
    "CART can perform both classification and regression tasks and they create decision points by considering Gini index unlike ID3 or C4.5 which uses information gain and gain ratio for splitting. For splitting, CART follows a greedy algorithm which aims only to reduce the cost function. For classification, cost function such as Gini index is used to indicate the purity of the leaf nodes. For regression, sum squared error is chosen by the algorithm as the cost function to find out the best prediction."
   ]
  },
  {
   "cell_type": "raw",
   "id": "68f3faa1",
   "metadata": {},
   "source": [
    "CHAID or Chi-square Automatic Interaction Detector is a process which can deal with any type of variables be it nominal, ordinal or continuous. In regression tree, it uses F-test and in classification trees, it uses the Chi-Square test.  In this analysis, continuous predictors are separated into equal number of observations until an outcome is achieved. It is very less used and adopted in real world problems compared to other algorithms."
   ]
  },
  {
   "cell_type": "raw",
   "id": "02450659",
   "metadata": {},
   "source": [
    "MARS or Multivariate adaptive regression splines is an analysis specially implemented in regression problems when the data is mostly nonlinear in nature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417602a7",
   "metadata": {},
   "source": [
    "## 4. What are Parametric and Nonparametric Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9e9f2ca0",
   "metadata": {},
   "source": [
    "Parametric Machine Learning Algorithms:\n",
    "Assumptions can greatly simplify the learning process, but can also limit what can be learned. Algorithms that simplify the function to a known form are called parametric machine learning algorithms.\n",
    "\n",
    "A learning model that summarizes data with a set of parameters of fixed size (independent of the number of training examples) is called a parametric model. No matter how much data you throw at a parametric model, it won’t change its mind about how many parameters it needs.\n",
    "\n",
    "The algorithms involve two steps:\n",
    "1.Select a form for the function.\n",
    "2.Learn the coefficients for the function from the training data.\n",
    "\n",
    "An easy to understand functional form for the mapping function is a line, as is used in linear regression:b0 + b1*x1 + b2*x2 = 0\n",
    "Where b0, b1 and b2 are the coefficients of the line that control the intercept and slope, and x1 and x2 are two input variables.\n",
    "Assuming the functional form of a line greatly simplifies the learning process. Now, all we need to do is estimate the coefficients of the line equation and we have a predictive model for the problem.\n",
    "Often the assumed functional form is a linear combination of the input variables and as such parametric machine learning algorithms are often also called “linear machine learning algorithms“.\n",
    "\n",
    "Some more examples of parametric machine learning algorithms include:\n",
    "Logistic Regression\n",
    "Linear Discriminant Analysis\n",
    "Perceptron\n",
    "Naive Bayes\n",
    "Simple Neural Networks\n",
    "\n",
    "Benefits of Parametric Machine Learning Algorithms:\n",
    "1.Simpler: These methods are easier to understand and interpret results.\n",
    "2.Speed: Parametric models are very fast to learn from data.\n",
    "3.Less Data: They do not require as much training data and can work well even if the fit to the data is not perfect.\n",
    "\n",
    "Limitations of Parametric Machine Learning Algorithms:\n",
    "1.Constrained: By choosing a functional form these methods are highly constrained to the specified form.\n",
    "2.Limited Complexity: The methods are more suited to simpler problems.\n",
    "3.Poor Fit: In practice the methods are unlikely to match the underlying mapping function."
   ]
  },
  {
   "cell_type": "raw",
   "id": "c53f8ffe",
   "metadata": {},
   "source": [
    "Nonparametric Machine Learning Algorithms:\n",
    "\n",
    "Algorithms that do not make strong assumptions about the form of the mapping function are called nonparametric machine learning algorithms. By not making assumptions, they are free to learn any functional form from the training data.\n",
    "\n",
    "Nonparametric methods are good when you have a lot of data and no prior knowledge, and when you don’t want to worry too much about choosing just the right features.\n",
    "Nonparametric methods seek to best fit the training data in constructing the mapping function, whilst maintaining some ability to generalize to unseen data. As such, they are able to fit a large number of functional forms.\n",
    "\n",
    "An easy to understand nonparametric model is the k-nearest neighbors algorithm that makes predictions based on the k most similar training patterns for a new data instance. The method does not assume anything about the form of the mapping function other than patterns that are close are likely to have a similar output variable.\n",
    "\n",
    "Some more examples of popular nonparametric machine learning algorithms are:\n",
    "k-Nearest Neighbors\n",
    "Decision Trees like CART and C4.5\n",
    "Support Vector Machines\n",
    "\n",
    "Benefits of Nonparametric Machine Learning Algorithms:\n",
    "1.Flexibility: Capable of fitting a large number of functional forms.\n",
    "2.Power: No assumptions (or weak assumptions) about the underlying function.\n",
    "3.Performance: Can result in higher performance models for prediction\n",
    "\n",
    "Limitations of Nonparametric Machine Learning Algorithms:\n",
    "1.More data: Require a lot more training data to estimate the mapping function.\n",
    "2.Slower: A lot slower to train as they often have far more parameters to train.\n",
    "3.Overfitting: More of a risk to overfit the training data and it is harder to explain why specific predictions are made."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee35814",
   "metadata": {},
   "source": [
    "## 5. Explain Decision tree Key terms: Root Node, Decision Node/Branch Node, Leaf or Terminal Node."
   ]
  },
  {
   "cell_type": "raw",
   "id": "6d9e835c",
   "metadata": {},
   "source": [
    "Root Node:\n",
    "It is the node present at the beginning of a decision tree from this node splitting starts according to various features.\n",
    "it is impure node\n",
    "\n",
    "Decision Nodes:\n",
    "the nodes we get after splitting the root nodes are called Decision Node\n",
    "\n",
    "Leaf Nodes:\n",
    "the nodes where further splitting is not possible are called leaf nodes or terminal nodes\n",
    "it is pure node\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ca7e35",
   "metadata": {},
   "source": [
    "## 6. What are Assumptions while creating a Decision Tree?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "38bb5af1",
   "metadata": {},
   "source": [
    "1.Discretization of continuous variables is required\n",
    "2.The data taken for training should be wholly considered as root\n",
    "3.Distribution of records is done in a recursive manner on the basis of attribute values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f181c0",
   "metadata": {},
   "source": [
    "## 7. What is entropy?\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b80bc6ff",
   "metadata": {},
   "source": [
    "It is defined as a measure of impurity present in the data. The entropy is almost zero when the sample attains homogeneity but is one when it is equally divided. Entropy with the lowest value makes a model better in terms of prediction as it segregates the classes better. Entropy is calculated based on the following formula:\n",
    "E(S) = - (P+) (logP+) - (P-)(logP-)\n",
    "Here,\n",
    "P+ is the probability of positive class\n",
    "P– is the probability of negative class\n",
    "S is the subset of the training example\n",
    "\n",
    "Entropy tends to be maximum in the middle with value up to 1 and minimum at the ends with value up to 0.\n",
    "A pure sub-split means that either you should be getting “yes”, or you should be getting “no”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2216edc1",
   "metadata": {},
   "source": [
    "## 8. What is Information Gain?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e718bfec",
   "metadata": {},
   "source": [
    "It is a measure used to generalize the impurity which is entropy in a dataset. Higher the information gain, lower is the entropy. An event having low probabilities to occur has lower entropy and high information whereas an event having high probabilities has higher entropy and low information. It is calculated as\n",
    "\n",
    "Information Gain = Entropy of Parent – sum (weighted % * Entropy of Child)\n",
    "\n",
    "Weighted % = Number of observations in particular child/sum (observations in all\n",
    "\n",
    "  child nodes)\n",
    "  \n",
    " A commonly used measure of purity is called information.\n",
    "For each node of the tree, the information value measures how much information a feature gives us about the class. The split with the highest information gain will be taken as the first split and the process will continue until all children nodes are pure, or until the information gain is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec5a67b",
   "metadata": {},
   "source": [
    "## 9. What is Gini Index?\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6db04b6a",
   "metadata": {},
   "source": [
    "It is a measure of misclassification and is used when the data contain multi class labels. Gini is similar to entropy but it calculates much quicker than entropy. Algorithms like CART (Classification and Regression Tree) use Gini as an impurity parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c476b5",
   "metadata": {},
   "source": [
    "## 10. What is a Puresubset?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f4d1be61",
   "metadata": {},
   "source": [
    "A pure subset is a subset that contains only samples of one class.\n",
    "The pure subset is a situation where we will get either all yes or all no."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3d48fe",
   "metadata": {},
   "source": [
    "## 11. What is the difference between Entropy and Gini Impurity(Gini Index)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2f4bbadb",
   "metadata": {},
   "source": [
    "1.The maximum value for entropy is 1 whereas the maximum value for Gini impurity is 0.5.\n",
    "2.As the Gini Impurity does not contain any logarithmic function to calculate it takes less computational time as compared to entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d71111",
   "metadata": {},
   "source": [
    "## 12. How do you calculate the entropy of children nodes after the split based on a feature?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7ee29efc",
   "metadata": {},
   "source": [
    "ID3 algorithm uses entropy to calculate the homogeneity of a sample. If the sample is completely homogeneous the entropy is zero and if the sample is an equally divided it has entropy of one. The information gain is based on the decrease in entropy after a dataset is split on an attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f305c84b",
   "metadata": {},
   "source": [
    "## 13. Does Decision Tree require feature scaling?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ebd42d8c",
   "metadata": {},
   "source": [
    "Decision trees and ensemble methods do not require feature scaling to be performed as they are not sensitive to the the variance in the data.\n",
    "Decision trees classification is not impacted by the outliers in the data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cab98a3",
   "metadata": {},
   "source": [
    "## 14. Explain feature selection using the information gain/entropy technique?\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e3cb81dc",
   "metadata": {},
   "source": [
    "What is Feature Selection?\n",
    "Feature selection is a technique used in machine learning to select the most relevant subset of available features in a dataset. This is a way to reduce the noise from the data and make sure the prediction/classification is more accurate.\n",
    "Does more features mean more information?\n",
    "This is not always the case. More information comes with more noise.Often there are many irrelevant features in a dataset that do not carry much of the information. These features, when not dealt with, fool the machine learning algorithms. This is sometimes also referred to as the curse of dimensionality. To solve the curse of dimensionality, we need to rank the features which affect the target most. This process is called feature selection.\n",
    "\n",
    "Information Gain method is also used in the decision tree algorithm to decide the splitting criteria.\n",
    "we will have numerous features X1, X2, X3……..Xn in dataset. In that case, we would determine the information gain for each of the features, IG(X1), IG(X2), …., and so on. We would then rank the features in the descending order of their respective information gains. We would decide a threshold and would include all the features above the threshold in the machine learning algorithms. \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3004f439",
   "metadata": {},
   "source": [
    "1.choose the attribute with highest Information Gain from the set as root node\n",
    "2.construct child node for each value of root node\n",
    "3.repeat recursively until the whole tree is built"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e02e5b1",
   "metadata": {},
   "source": [
    "## 15. What are Techniques to avoid Overfitting in Decision Tree?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a234d543",
   "metadata": {},
   "source": [
    "There might also be a possibility of overfitting when the branches involve features that have very low importance. Overfitting can be avoided by two methods"
   ]
  },
  {
   "cell_type": "raw",
   "id": "85fd5902",
   "metadata": {},
   "source": [
    "1.Pruning\n",
    "Pruning is a process of chopping down the branches which consider features having low importance.  It either begins from root or from leaves where it removes the nodes having the most popular class. Other methods include adding a parameter to decide removing a node on the basis of the size of the sub tree. This method is simply known as post pruning. On the other hand, pre pruning is the method which stops the tree making decisions by producing leaves considering smaller samples. As the name suggests, it should be done at an early stage to avoid overfitting."
   ]
  },
  {
   "cell_type": "raw",
   "id": "a0075a01",
   "metadata": {},
   "source": [
    "2. Ensemble method or bagging and boosting\n",
    "Ensemble method like a random forest is used to overcome overfitting by resampling training data repeatedly building multiple decision trees. Boosting technique is also a powerful method which is used both in classification and regression problems where it trains new instances to give importance to those instances which are misclassified. AdaBoost is one commonly used boosting technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c20b152",
   "metadata": {},
   "source": [
    "## 16. How to tune hyperparameters in decision trees Classifier?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "448263eb",
   "metadata": {},
   "source": [
    "Methods used to tune hyperparameters:\n",
    "1.Grid Search: This involves an exhaustive search through a list of all possible hyperparameter values to try. Every possible combination is tried, and then the best performing model can be selected. While this option tends to perform well, it is computationally expensive\n",
    "2.Randomised Search:  In this approach, a random set of samples are selected from the hyperparameter space. We can define the hyperparameter space with lists (like in Grid Search), or with statistical distributions. Randomised Search is much more computationally efficient, but can yield lower quality results if we are not careful with the parameter space definition\n",
    "3.Bayesian Search: Here we build a probabilistic model to identify the best set of hyperparameters. The hyperparameter space is defined by statistical distributions. We can further influence how the tuning performs through a careful selection of prior distributions. This method is also computationally efficient, but it is more complex to use or explain, when compared with Grid Search or Randomised Search"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b0a3a0fa",
   "metadata": {},
   "source": [
    "some parameters:\n",
    "1.max_depth: We can set the maximum depth of our decision tree using the max_depth parameter. The more the value of max_depth, the more complex your tree will be. The training error will off-course decrease if we increase the max_depth value but when our test data comes into the picture, we will get a very bad accuracy. Hence you need a value that will not overfit as well as underfit our data and for this, you can use GridSearchCV.\n",
    "2.min_samples_split: Another way is to set the minimum number of samples for each spilt. It is denoted by min_samples_split. Here we specify the minimum number of samples required to do a spilt. For example, we can use a minimum of 10 samples to reach a decision. That means if a node has less than 10 samples then using this parameter, we can stop the further splitting of this node and make it a leaf node.\n",
    "3.min_samples_leaf: represents the minimum number of samples required to be in the leaf node. The more you increase the number, the more is the possibility of overfitting.\n",
    "4.max_features: it helps us decide what number of features to consider when looking for the best split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4a2ad7",
   "metadata": {},
   "source": [
    "## 17. What is pruning in the Decision Tree?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1d56385e",
   "metadata": {},
   "source": [
    "In Decision Tree pruning removes the branches of decision tree to overcome the overfitting condition"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7bc74140",
   "metadata": {},
   "source": [
    "1. Post Pruning :\n",
    "This technique is used after construction of decision tree.\n",
    "This technique is used when decision tree will have very large depth and will show overfitting of model.\n",
    "It is also known as backward pruning.\n",
    "This technique is used when we have infinitely grown decision tree.\n",
    "Here we will control the branches of decision tree that is max_depth and min_samples_split using cost_complexity_pruning"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c5c3d7a5",
   "metadata": {},
   "source": [
    "2. Pre-Pruning :\n",
    "This technique is used before construction of decision tree.\n",
    "Pre-Pruning can be done using Hyperparameter tuning.\n",
    "it Overcomes the overfitting issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596ed519",
   "metadata": {},
   "source": [
    "## 18. How is a splitting point chosen for continuous variables in decision trees?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ae800bbb",
   "metadata": {},
   "source": [
    "In order to come up with a split point, the values are sorted, and the mid-points between adjacent values are evaluated in terms of some metric, usually information gain or gini impurity.\n",
    "For example, lets say we have four examples and the values of the age variable are (20,29,40,50)\n",
    "The midpoints between the values (24.5,34.5,45) are evaluated, and whichever split gives the best information gain (or whatever metric you're using) on the training data is used.\n",
    "\n",
    "You can save some computation time by only checking split points that lie between examples of different classes, because only these splits can be optimal for information gain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254f63e5",
   "metadata": {},
   "source": [
    "## 19. What are the advantages and disadvantages of the Decision Tree?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b1da36c2",
   "metadata": {},
   "source": [
    "Advantages:\n",
    "1.Preprocessing of data such as normalization and scaling is not required which reduces the effort in building a model.\n",
    "2.A decision tree algorithm can handle both categorical and numeric data and is much efficient compared to other algorithms.\n",
    "3.Any missing value present in the data does not affect a decision tree which is why it is considered a flexible algorithm.\n",
    "4.They have high interpretability, which makes them the go-to algorithm for real-world business applications. They can be explained as a series of questions/ if-else statements."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f3359c5a",
   "metadata": {},
   "source": [
    "Disadvantages:\n",
    "1.A decision tree works badly when it comes to regression as it fails to perform if the data have too much variation.\n",
    "2.A decision tree is sometimes unstable and cannot be reliable as alteration in data can cause a decision tree go in a bad structure which may affect the accuracy of the model.\n",
    "3.If the data are not properly discretized, then a decision tree algorithm can give inaccurate results and will perform badly compared to other algorithms.\n",
    "4.Complexities arise in calculation if the outcomes are linked and it may consume time while training a model.\n",
    "5.They are often relatively inaccurate and prone to overfitting. This can be rectified by replacing a single decision tree with a random forest of decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c010481a",
   "metadata": {},
   "source": [
    "## 20. What is Decision Tree Regressor?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c08e401",
   "metadata": {},
   "source": [
    "Decision Tree Regression: \n",
    "Decision tree regression observes features of an object and trains a model in the structure of a tree to predict data in the future to produce meaningful continuous output. Continuous output means that the output/result is not discrete, i.e., it is not represented just by a discrete, known set of numbers or values.\n",
    "\n",
    "Discrete output example: A weather prediction model that predicts whether or not there’ll be rain on a particular day. \n",
    "Continuous output example: A profit prediction model that states the probable profit that can be generated from the sale of a product.\n",
    "Here, continuous values are predicted with the help of a decision tree regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e0a59c",
   "metadata": {},
   "source": [
    "## 21. How is Splitting Decided for Decision Trees Regressor?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d6e24a7b",
   "metadata": {},
   "source": [
    "Firstly calculate the variance for each child node.\n",
    "Then calculate the variance of each split as the weighted average variance for child nodes.\n",
    "Compare all the varaince and then select the split whose variance is the lowest.\n",
    "And then again follow the above 3 steps until you achieve homogeneous nodes.\n",
    "Note: If the node gets entirely homogeneous, then the variance is equal to zero."
   ]
  },
  {
   "cell_type": "raw",
   "id": "208296e2",
   "metadata": {},
   "source": [
    "To understand the concept of regression trees, we must have a clear idea about the concept of regression and linear regression.In distribution, where we can easily fit a line to it. So, we can use linear regression to predict a value. But, in the case of non-linear distribution, Regression Trees are used. \n",
    "In regression trees, the leaves represent a continuous numeric value in contrast to classification trees which normally represent boolean or discrete values at the leaves."
   ]
  },
  {
   "cell_type": "raw",
   "id": "12d6e476",
   "metadata": {},
   "source": [
    "A measure that tells us how much our predictions deviate from the original target that is mean square error.In the Regression Tree algorithm, we do the same thing as the Classification trees. But, we try to reduce the Mean Square Error at each child rather than the entropy."
   ]
  },
  {
   "cell_type": "raw",
   "id": "293a489b",
   "metadata": {},
   "source": [
    "Let’s consider a dataset where we have 2 variables.we need to build a Regression tree that best predicts the Y given the X.\n",
    "X 1  2   3   4  5  6   7   8   9  10  11  12 13  14\n",
    "y 1 1.2 1.4 1.1 1 5.5 6.1 6.7 6.4  6   6  3  3.2 3.1\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c72d650d",
   "metadata": {},
   "source": [
    "Step 1\n",
    "The first step is to sort the data based on X ( In this case, it is already sorted ). Then, take the average of the first 2 rows in variable X ( which is (1+2)/2 = 1.5 according to the given dataset ). Divide the dataset into 2 parts ( Part A and Part B ) , separated by x < 1.5 and X ≥ 1.5.\n",
    "\n",
    "Now, Part A consist only of one point, which is the first row (1,1) and all the other points are in Part — B. Now, take the average of all the Y values in Part A and average of all Y values in Part B separately. These 2 values are the predicted output of the decision tree for x < 1.5 and x ≥ 1.5 respectively. Using the predicted and original values, calculate the mean square error and note it down."
   ]
  },
  {
   "cell_type": "raw",
   "id": "65d01738",
   "metadata": {},
   "source": [
    "Step 2\n",
    "In step 1, we calculated the average for the first 2 numbers of sorted X and split the dataset based on that and calculated the predictions. Then, we do the same process again but this time, we calculate the average for the second 2 numbers of sorted X ( (2+3)/2 = 2.5 ). Then, we split the dataset again based on X < 2.5 and X ≥ 2.5 into Part A and Part B again and predict outputs, find mean square error as shown in step 1. This process is repeated for the third 2 numbers, the fourth 2 numbers, the 5th, 6th, 7th till n-1th 2 numbers ( where n is the number of records or rows in the dataset )."
   ]
  },
  {
   "cell_type": "raw",
   "id": "656a8382",
   "metadata": {},
   "source": [
    "Step 3\n",
    "Now that we have n-1 mean squared errors calculated , we need to choose the point at which we are going to split the dataset. and that point is the point, which resulted in the lowest mean squared error on splitting at it. In this case, the point is x=5.5. Hence the tree will be split into 2 parts. x<5.5 and x≥ 5.5. The Root node is selected this way and the data points that go towards the left child and right child of the root node are further recursively exposed to the same algorithm for further splitting."
   ]
  },
  {
   "cell_type": "raw",
   "id": "6c1c5d74",
   "metadata": {},
   "source": [
    "Brief Explanation of What the algorithm is doing:\n",
    "The basic idea behind the algorithm is to find the point in the independent variable to split the data-set into 2 parts, so that the mean squared error is the minimised at that point. The algorithm does this in a repetitive fashion and forms a tree-like structure."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ba42be0a",
   "metadata": {},
   "source": [
    "All we are doing is splitting the data-set by selecting certain points that best splits the data-set and minimises the mean square error.\n",
    "And the way we are selecting these points is by going through an iterative process of calculating mean square error for all the splits and choosing the split that has the least value for the mse."
   ]
  },
  {
   "cell_type": "raw",
   "id": "6b871032",
   "metadata": {},
   "source": [
    "What happens when there are multiple independent variables ?\n",
    "Let us consider that there are 3 variables similar to the independent variable X \n",
    "At each node, All the 3 variables would go through the same process as what X went through in the above example. The data would be sorted based on the 3 variables separately.\n",
    "The points that minimises the mse are calculated for all the 3 variables. out of the 3 variables and the points calculated for them, the one that has the least mse would be chosen.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4f679c3c",
   "metadata": {},
   "source": [
    "How are categorical variables handled ?\n",
    "When we use the continuous variables as independent variables , we select the point with the least mse using an iterative algorithm as mentioned above. When given a categorical variable , we simply split it by asking a binary question ( usually ). For example, let’s say we have a column specifying the size of the tumor in categorical terms. say Small, Medium and Large.\n",
    "The tree would split the data-set based on whether tumor size = small or tumor size = large or tumor size = Medium or it can also combine multiple values in some cases, based on whichever question reduces the mse the most. and that becomes the top contender for this variable (Tumor Size). The Top contenders of the other variables are compared with this and the selection process is similar to the situation mentioned in “What happens when there are multiple independent variables ?”"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1ed10dff",
   "metadata": {},
   "source": [
    "Dealing with Over-Fitting and When to stop building the Tree ?\n",
    "Regression Trees are prone to this problem.\n",
    "When we want to reduce the mean square error, the decision tree can recursively split the data-set into a large number of subsets to the the point where a set contains only one row or record. Even though this might reduce the mse to zero, this is obviously not a good thing.\n",
    "\n",
    "This is the famous problem of overfitting and the models fit to the existing data too perfectly that it fails to generalise with new data. We can use cross validation methods to avoid this.\n",
    "\n",
    "One way to prevent this, with respect to Regression trees, is to specify the minimum number of records or rows, A leaf node can have, In advance.\n",
    "And the exact number is not easily known when it comes to large data-sets. But, cross-validation could be used for this purpose."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ab61a98d",
   "metadata": {},
   "source": [
    "Regression Trees are another way of calling Decision Trees that are used for regression and it can be useful in a lot of areas where the relationship between the variables are found to be non-linear.\n",
    "One thing to remember is that the algorithm is prone to overfitting. So, It is better to always specify the minimum number of children per leaf node in advance and use cross-validation to find this Value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6971988",
   "metadata": {},
   "source": [
    "## 22. Explain Linear regression vs decision trees regression"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cc2fd19c",
   "metadata": {},
   "source": [
    "1.Linear Regression is used to predict continuous outputs where there is a linear relationship between the features of the dataset and the output variable. It is used for regression problems where you are trying to predict something with infinite possible answers such as the price of a house.\n",
    "2.Linear regression gives a continuous output and is used for regression tasks. It can be used when the independent variables (the factors that you want to use to predict with) have a linear relationship with the output variable (what you want to predict) i.e it is of the form Y= C+aX1+bX2 (linear)\n",
    "3.If the relationship between X and Y looks linear in a scatterplot, then a linear regression will represent the data better.\n",
    "4.Linear regression however, is a very robust model meaning that a small change in the sample data will not affect the linear regression equation so much (especially if you followed the 10: 1 rule, which states that you need 10 observations for each predictor in the model)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "67e34a39",
   "metadata": {},
   "source": [
    "1.Decision trees can be used for either classification or regression problems and are useful for complex datasets. They work by splitting the dataset, in a tree-like structure, into smaller and smaller subsets and then make predictions based on what subset a new example would fall into.\n",
    "2.In the case of regression, decision trees learn by splitting the training examples in a way such that the sum of squared residuals is minimized. It then predicts the output value by taking the average of all of the examples that fall into a certain leaf on the decision tree and using that as the output prediction.\n",
    "3.If the relationship is non-linear between X and Y, then a regression tree will probably fit the data better.\n",
    "4.The regression tree is a flexible model meaning that a small change in the sample data will result in a large change in the model, so validating the model using a separate dataset (or cross-validation) is very important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d4487f",
   "metadata": {},
   "source": [
    "## 23. How to tune hyperparameters in decision trees regression?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0cdfdb3b",
   "metadata": {},
   "source": [
    "Grid Search\n",
    "The first hyperparameter tuning technique we will try is Grid Search. For both the classification and regression cases, we will define the parameter space, and then make use of scikit-learn’s GridSearchCV. Each parameter configuration will be validated using 5-fold Cross-Validation. Afterwards, the best model will be selected, and tested against our held-out test set.\n",
    "\n",
    "Randomised Search\n",
    "The second hyperparameter tuning technique we will try is Randomised Search. For both the classification and regression cases, we will define the parameter space, and then make use of scikit-learn’s RandomizedSearchCV. The tuner will sample 100 different parameter configurations, where each will be validated using 5-fold Cross-Validation. Afterwards, the best model will be selected, and tested against our held-out test set.\n",
    "\n",
    "Bayesian Search\n",
    "The last hyperparameter tuning technique we will try is Bayesian Search. For both the classification and regression cases, we will define the parameter space, and then make use of scikit-optimize’s BayesSearchCV. The tuner will sample 10 different parameter configurations, where each will be validated using 5-fold Cross-Validation. Afterwards, the best model will be selected, and tested against our held-out test set.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "05e39e14",
   "metadata": {},
   "source": [
    "#GridSearchCV example\n",
    "# setup parameter space\n",
    "parameters = {'criterion':['squared_error','absolute_error'],\n",
    "              'max_depth':np.arange(1,21).tolist()[0::2],\n",
    "              'min_samples_split':np.arange(2,11).tolist()[0::2],\n",
    "              'max_leaf_nodes':np.arange(3,26).tolist()[0::2]}\n",
    "\n",
    "# create an instance of the grid search object\n",
    "g2 = GridSearchCV(DecisionTreeRegressor(), parameters, cv=5, n_jobs=-1)\n",
    "\n",
    "# conduct grid search over the parameter space\n",
    "start_time = time.time()\n",
    "g2.fit(X2_train,y2_train)\n",
    "duration = time.time() - start_time\n",
    "\n",
    "# show best parameter configuration found for regressor\n",
    "rgr_params1 = g2.best_params_\n",
    "rgr_params1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2a8bb5f2",
   "metadata": {},
   "source": [
    "# compute performance on test set\n",
    "model = g2.best_estimator_\n",
    "y_pred = model.predict(X2_test)\n",
    "print('mse score: %.2f' % mean_squared_error(y2_test,y_pred))\n",
    "print('mae score: %.2f' % mean_absolute_error(y2_test,y_pred))\n",
    "print('computation time: %.2f' % duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e73b59",
   "metadata": {},
   "source": [
    "## 24. What is max_depth in the Decision Tree?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "96640b91",
   "metadata": {},
   "source": [
    "max_depth: int or None, optional (default=None)\n",
    "The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "\n",
    "The theoretical maximum depth a decision tree can achieve is one less than the number of training samples, but no algorithm will let you reach this point for obvious reasons, one big reason being overfitting. Note here that it is the number of training samples and not the number of features because the data can be split on the same feature multiple times.\n",
    "\n",
    "Let’s first talk about the default None case, if you don’t specify a depth for the tree, scikit-learn will expand the nodes until all leaves are pure, meaning the leaf will only have labels if you choose default for the min_samples_leaf, where the default value is one. Note that most of these hyperparameters are tied to one another and we will talk about the min_samples_leaf shortly. On the other hand, if you specify a min_samples_split,  the nodes will be expanded until all leaves contain less than the minimum number of samples. Scikit-learn will pick one over the other depending on which gives the maximum depth for your tree. There’s a lot of moving parts here, min_samples_split and min_samples_leaf so let’s just take the max_depth in isolation and see what happens to your model when you change it, so after we go through min_samples_split and min_samples_leaf we can get a better intuition of how all these come together.\n",
    "\n",
    "In general, the deeper you allow your tree to grow, the more complex your model will become because you will have more splits and it captures more information about the data and this is one of the root causes of overfitting in decision trees because your model will fit perfectly for the training data and will not be able to generalize well on test set. So, if your model is overfitting, reducing the number for max_depth is one way to combat overfitting.\n",
    "\n",
    "It is also bad to have a very low depth because your model will underfit so how to find the best value? experiment because overfitting and underfitting are very subjective to a dataset, there is no one value fits all solution. So what I usually do is, let the model decide the max_depth first and then by comparing my train and test scores I look for overfitting or underfitting and depending on the degree I decrease or increase the max_depth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc77ff2",
   "metadata": {},
   "source": [
    "## 25. What are the min_samples_split and min_samples_leaf hyperparameters?\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2f2e0a90",
   "metadata": {},
   "source": [
    "min_samples_split: int, float, optional (default=2)\n",
    "\n",
    "The minimum number of samples required to split an internal node:\n",
    "\n",
    "If int, then consider min_samples_split as the minimum number.\n",
    "If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.\n",
    "\n",
    "min_samples_split and min_samples_leaf, if you read their definitions it sounds like one would imply the other, but what you need to note is a leaf is an external node and the min_samples_split talks about an internal node and by definition an internal node can have further split whereas a leaf node by definition is a node without any children.\n",
    "\n",
    "Say you specify a min_samples_split and the resulting split results in a leaf with 1 sample and you have specified min_samples_leaf as 2, then your min_samples_split will not be allowed. In other words, min_samples_leaf is always guaranteed no matter the min_samples_split value.\n",
    "\n",
    "According to the paper, An empirical study on hyperparameter tuning of decision trees [5] the ideal min_samples_split values tend to be between 1 to 40 for the CART algorithm which is the algorithm implemented in scikit-learn. min_samples_split is used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree. Too high values can also lead to under-fitting hence depending on the level of underfitting or overfitting, you can tune the values for min_samples_split.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c98f59c",
   "metadata": {},
   "source": [
    "min_samples_leaf: int, float, optional (default=1)\n",
    "\n",
    "The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.\n",
    "\n",
    "If int, then consider min_samples_leaf as the minimum number.\n",
    "If float, then min_samples_leaf is a fraction and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.\n",
    "\n",
    "Similar to min_samples_split, min_samples_leaf is also used to control over-fitting by defining that each leaf has more than one element. Thus ensuring that the tree cannot overfit the training dataset by creating a bunch of small branches exclusively for one sample each. In reality, what this is actually doing is simply just telling the tree that each leaf doesn’t have to have an impurity of 0.\n",
    "\n",
    "The paper, An empirical study on hyperparameter tuning of decision trees [5] also states that the ideal min_samples_leaf values tend to be between 1 to 20 for the CART algorithm. This paper also indicates that min_samples_split and min_samples_leaf are the most responsible for the performance of the final trees from their relative importance analysis [5].\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f455a6",
   "metadata": {},
   "source": [
    "## 26. What are the Applications of Decision Trees?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "492b69a1",
   "metadata": {},
   "source": [
    "1.In healthcare industries \n",
    "In healthcare industries, decision tree can tell whether a patient is suffering from a disease or not based on conditions such as age, weight, sex and other factors. Other applications such as deciding the effect of the medicine based on factors such as composition, period of manufacture, etc. Also, in diagnosis of medical reports, a decision tree can be very effective.\n",
    "2. In banking sectors.\n",
    "A person eligible for a loan or not based on his financial status, family member, salary, etc. can be decided on a decision tree. Other applications may include credit card frauds, bank schemes and offers, loan defaults, etc. which can be prevented by using a proper decision tree.\n",
    "3. In educational Sectors\n",
    "In colleges and universities, the shortlisting of a student can be decided based upon his merit scores, attendance, overall score etc. A decision tree can also decide the overall promotional strategy of faculties present in the universities. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
